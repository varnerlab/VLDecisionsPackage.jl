function lookahead(p::MyMDPProblemModel, U::Vector{Float64}, s::Int64, a::Int64)

    # grab stuff from the problem -
    R = p.R;  # reward -
    T = p.T;    
    Î³ = p.Î³;
    ğ’® = p.ğ’®;
    
    # setup my state array -
    return R[s,a] + Î³*sum(T[s,sâ€²,a]*U[i] for (i,sâ€²) in enumerate(ğ’®))
end

function lookahead(p::MyMDPProblemModel, U::Function, s::Int64, a::Int64)

    # get data from the problem -
    ğ’®, T, R, Î³ = p.ğ’®, p.T, p.R, p.Î³;
    return R(s,a) + Î³*sum(T(s,sâ€²,a)*U(sâ€²) for sâ€² in ğ’®)
end

function iterative_policy_evaluation(p::MyMDPProblemModel, Ï€, k_max::Int)

    # grab stuff from the problem -
    R = p.R;  # reward -
    T = p.T;    
    Î³ = p.Î³;
    ğ’® = p.ğ’®;

    # initialize value -
    U = [0.0 for s âˆˆ ğ’®];

    for _ âˆˆ 1:k_max
        U = [lookahead(p, U, s, Ï€(s)) for s âˆˆ ğ’®]
    end

    return U;
end

function Q(p::MyMDPProblemModel, U::Array{Float64,1})::Array{Float64,2}

    # grab stuff from the problem -
    R = p.R;  # reward -
    T = p.T;    
    Î³ = p.Î³;
    ğ’® = p.ğ’®;
    ğ’œ = p.ğ’œ

    # initialize -
    Q_array = Array{Float64,2}(undef, length(ğ’®), length(ğ’œ))

    for s âˆˆ 1:length(ğ’®)
        for a âˆˆ 1:length(ğ’œ)
            Q_array[s,a] = R[s,a] + Î³*sum([T[s,sâ€²,a]*U[sâ€²] for sâ€² in ğ’®]);
        end
    end

    return Q_array
end

function policy(Q_array::Array{Float64,2})::Array{Int64,1}

    # get the dimension -
    (NR, _) = size(Q_array);

    # initialize some storage -
    Ï€_array = Array{Int64,1}(undef, NR)
    for s âˆˆ 1:NR
        Ï€_array[s] = argmax(Q_array[s,:]);
    end

    # return -
    return Ï€_array;
end

function backup(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64)
    return maximum(lookahead(problem, U, s, a) for a âˆˆ problem.ğ’œ);
end

function solve(model::MyValueIterationModel, problem::MyMDPProblemModel)::MyValueFunctionPolicy
    
    # data -
    k_max = model.k_max;

    # initialize
    U = [0.0 for _ âˆˆ problem.ğ’®];

    # main loop -
    for _ âˆˆ 1:k_max
        U = [backup(problem, U, s) for s âˆˆ problem.ğ’®];
    end

    return MyValueFunctionPolicy(problem, U);
end

function greedy(problem::MyMDPProblemModel, U::Array{Float64,1}, s::Int64)
    u, a = findmax(a->lookahead(problem, U, s, a), problem.ğ’œ);
    return (a=a, u=u)
end

(Ï€::MyValueFunctionPolicy)(s::Int64) = greedy(Ï€.problem, Ï€.U, s).a;

function rollout(problem::MyMDPProblemModel, s::Int64, policy::Function, depth::Int64)::Float64

    # initialize -
    ret = 0.0;
    for i âˆˆ 1:depth
        a = policy(s);
        s, r = randstep(problem, s, a);
        ret += problem.Î³^(i-1)*r;
    end
    return ret;
end

randstep(problem::MyMDPProblemModel, s::Int64, a::Int64)::Int64 = problem.TR(s,a)

function (Ï€::MyRolloutLookaheadModel)(s::Int64)
    U(s) = lookahead(Ï€.problem, s, Ï€.policy, Ï€.depth);
    return greedy(Ï€.problem, U, s).a;
end
